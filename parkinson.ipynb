{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34283830",
   "metadata": {},
   "source": [
    "# Parkinson Disease Detection from Audio using Deep Learning Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33046d36",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a65325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Reshape, LSTM, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras_tuner import Hyperband\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a232c02d",
   "metadata": {},
   "source": [
    "## Reading and Converting Data to MFCCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d960fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "parkinson_dir = \"train_data/parkinson\"\n",
    "healthy_dir = \"train_data/healthy\"\n",
    "\n",
    "parkinson_dir_test = \"test_data/parkinson\"\n",
    "healthy_dir_test = \"test_data/healthy\"\n",
    "\n",
    "\n",
    "def extract_mfcc(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13, n_fft=100, hop_length=50)\n",
    "    return mfcc\n",
    "\n",
    "\n",
    "def load_data_from_dir(folder):\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            file_path = os.path.join(folder, filename)\n",
    "            mfcc = extract_mfcc(file_path)\n",
    "            mfccs.append(mfcc)\n",
    "            labels.append(1 if \"parkinson\" in folder else 0)\n",
    "    return mfccs, labels\n",
    "\n",
    "\n",
    "parkinson_mfccs, parkinson_labels = load_data_from_dir(parkinson_dir)\n",
    "healthy_mfccs, healthy_labels = load_data_from_dir(healthy_dir)\n",
    "\n",
    "parkinson_mfccs_test, parkinson_labels_test = load_data_from_dir(parkinson_dir_test)\n",
    "healthy_mfccs_test, healthy_labels_test = load_data_from_dir(healthy_dir_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = min(min(mfcc.shape[1] for mfcc in parkinson_mfccs), min(mfcc.shape[1] for mfcc in healthy_mfccs))\n",
    "\n",
    "def truncate_mfccs(parkinson_mfccs, healthy_mfccs):\n",
    "    return [mfcc[:, :min_length] for mfcc in parkinson_mfccs], [mfcc[:, :min_length] for mfcc in healthy_mfccs] \n",
    "\n",
    "\n",
    "parkinson_mfccs, healthy_mfccs = truncate_mfccs(parkinson_mfccs, healthy_mfccs)\n",
    "parkinson_mfccs_test, healthy_mfccs_test = truncate_mfccs(parkinson_mfccs_test, healthy_mfccs_test)\n",
    "\n",
    "\n",
    "X_train = np.concatenate((parkinson_mfccs, healthy_mfccs), axis=0)\n",
    "y_train = np.concatenate((parkinson_labels, healthy_labels), axis=0)\n",
    "\n",
    "X_test = np.concatenate((parkinson_mfccs_test, healthy_mfccs_test), axis=0)\n",
    "y_test = np.concatenate((parkinson_labels_test, healthy_labels_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eab2f2",
   "metadata": {},
   "source": [
    "## Preparing Data for Deep Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1335ae29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2dd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b4e95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7314cd81",
   "metadata": {},
   "source": [
    "## Deep Learning Modeling and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d604fe7",
   "metadata": {},
   "source": [
    "### ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ann = X_train.reshape(81, 13 * 243)\n",
    "X_test_ann = X_test.reshape(20, 13 * 243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436521c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(13 * 243,)))\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 5)):\n",
    "\n",
    "        model.add(Dense(hp.Int(f'units_{i}', min_value=64, max_value=512, step=32), activation='relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586de05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,\n",
    "    factor=3,\n",
    "    directory='tuners',\n",
    "    project_name='ann_tuners'\n",
    ")\n",
    "\n",
    "tuner.search(X_train_ann, y_train, validation_data=(X_test_ann, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf5bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = tuner.get_best_models(num_models=1)[0]\n",
    "ann.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc100afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = ann.fit(X_train_ann, y_train, epochs=15, validation_data=(X_test_ann, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.save(\"models/ann.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2801a77",
   "metadata": {},
   "source": [
    "### ANN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb631222",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Loss and Validation Loss over Epochs')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_ylim(0, 60)\n",
    "ax1.set_xlim(0, 15)\n",
    "ax1.set_xticks(np.arange(0, 15, 1))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy and Validation Accuracy over Epochs')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_xlim(0, 15)\n",
    "ax2.set_xticks(np.arange(0, 15, 1))\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7274c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('models/ann_accuracy_loss_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eec8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = load_model(\"models/ann.keras\")\n",
    "\n",
    "y_train_pred = (ann.predict(X_train_ann) > 0.5).astype(\"int32\")\n",
    "y_test_pred = (ann.predict(X_test_ann) > 0.5).astype(\"int32\")\n",
    "\n",
    "ann_train_confusion_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "ann_train_accuracy = round(accuracy_score(y_train, y_train_pred), 2)\n",
    "ann_train_recall = round(recall_score(y_train, y_train_pred), 2)\n",
    "ann_train_precision = round(precision_score(y_train, y_train_pred), 2)\n",
    "ann_train_f1 = round(f1_score(y_train, y_train_pred), 2)\n",
    "\n",
    "ann_test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "ann_test_accuracy = round(accuracy_score(y_test, y_test_pred), 2)\n",
    "ann_test_recall = round(recall_score(y_test, y_test_pred), 2)\n",
    "ann_test_precision = round(precision_score(y_test, y_test_pred), 2)\n",
    "ann_test_f1 = round(f1_score(y_test, y_test_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d427eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.heatmap(ann_train_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Train Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(ann_test_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Test Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d95be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('models/ann_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8bf8d9",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78f421",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Input(shape=(13, 243, 1)))\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Conv2D(\n",
    "            filters=hp.Int('conv_1_filters', min_value=8, max_value=24, step=4),\n",
    "            padding='same',\n",
    "            kernel_size=(3, 3),\n",
    "            activation='relu',\n",
    "        ))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b1420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,\n",
    "    factor=3,\n",
    "    directory='tuners',\n",
    "    project_name='cnn_tuners'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae0ec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tuner.get_best_models(num_models=1)[0]\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c236bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270416c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.save(\"models/cnn.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e623f",
   "metadata": {},
   "source": [
    "### CNN Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f35254",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Loss and Validation Loss over Epochs')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_ylim(0, 40)\n",
    "ax1.set_xlim(0, 15)\n",
    "ax1.set_xticks(np.arange(0, 15, 1))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy and Validation Accuracy over Epochs')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.set_xlim(0, 15)\n",
    "ax2.set_xticks(np.arange(0, 15, 1))\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('models/cnn_accuracy_loss_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dd16ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = load_model(\"models/cnn.keras\")\n",
    "\n",
    "y_train_pred = (cnn.predict(X_train) > 0.5).astype(\"int32\")\n",
    "y_test_pred = (cnn.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "cnn_train_confusion_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "cnn_train_accuracy = round(accuracy_score(y_train, y_train_pred), 2)\n",
    "cnn_train_recall = round(recall_score(y_train, y_train_pred), 2)\n",
    "cnn_train_precision = round(precision_score(y_train, y_train_pred), 2)\n",
    "cnn_train_f1 = round(f1_score(y_train, y_train_pred), 2)\n",
    "\n",
    "cnn_test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "cnn_test_accuracy = round(accuracy_score(y_test, y_test_pred), 2)\n",
    "cnn_test_recall = round(recall_score(y_test, y_test_pred), 2)\n",
    "cnn_test_precision = round(precision_score(y_test, y_test_pred), 2)\n",
    "cnn_test_f1 = round(f1_score(y_test, y_test_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db9031c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.heatmap(cnn_train_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Train Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(cnn_test_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Test Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4666e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('models/cnn_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ac9c4",
   "metadata": {},
   "source": [
    "### CNN-LSTM Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5087e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    layer_count = 0\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Input(shape=(13, 243, 1)))\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 3)):\n",
    "        model.add(Conv2D(\n",
    "            hp.Int(f'filters_{i}', min_value=8, max_value=32, step=4),\n",
    "            (3, 3),\n",
    "            padding='same',\n",
    "            activation='relu'\n",
    "        ))\n",
    "        layer_count += 1\n",
    "    \n",
    "    model.add(Reshape((-1, 243)))\n",
    "    layer_count += 1\n",
    "\n",
    "    for i in range(hp.Int('num_layers', 1, 2)):\n",
    "        model.add(LSTM(\n",
    "            hp.Int(f'lstm_units_{layer_count + i}', min_value=8, max_value=32, step=4),\n",
    "            return_sequences=True\n",
    "        ))\n",
    "        layer_count += 1\n",
    "    \n",
    "    model.add(LSTM(\n",
    "        hp.Int(f'lstm_units{layer_count}', min_value=8, max_value=32, step=4),\n",
    "        return_sequences=False\n",
    "    ))\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float('learning_rate', min_value=1e-4, max_value=1e-2)),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e4d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=5,\n",
    "    factor=3,\n",
    "    directory='tuners',\n",
    "    project_name='hybrid_tuners'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801c407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid = tuner.get_best_models(num_models=1)[0]\n",
    "hybrid.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1f9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = hybrid.fit(X_train, y_train, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab1ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid.save(\"models/hybrid.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bd6db",
   "metadata": {},
   "source": [
    "### CNN-LSTM Hybrid Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83e019",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(history.history['loss'], label='Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss')\n",
    "ax1.set_title('Loss and Validation Loss over Epochs')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.set_xlim(0, 15)\n",
    "ax1.set_xticks(np.arange(0, 15, 1))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy and Validation Accuracy over Epochs')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.set_xlim(0, 15)\n",
    "ax2.set_xticks(np.arange(0, 15, 1))\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8b2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('models/hybrid_accuracy_loss_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid = load_model(\"models/hybrid.keras\")\n",
    "\n",
    "y_train_pred = (hybrid.predict(X_train) > 0.5).astype(\"int32\")\n",
    "y_test_pred = (hybrid.predict(X_test) > 0.5).astype(\"int32\")\n",
    "\n",
    "hybrid_train_confusion_matrix = confusion_matrix(y_train, y_train_pred)\n",
    "hybrid_train_accuracy = round(accuracy_score(y_train, y_train_pred), 2)\n",
    "hybrid_train_recall = round(recall_score(y_train, y_train_pred), 2)\n",
    "hybrid_train_precision = round(precision_score(y_train, y_train_pred), 2)\n",
    "hybrid_train_f1 = round(f1_score(y_train, y_train_pred), 2)\n",
    "\n",
    "hybrid_test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "hybrid_test_accuracy = round(accuracy_score(y_test, y_test_pred), 2)\n",
    "hybrid_test_recall = round(recall_score(y_test, y_test_pred), 2)\n",
    "hybrid_test_precision = round(precision_score(y_test, y_test_pred), 2)\n",
    "hybrid_test_f1 = round(f1_score(y_test, y_test_pred), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29babdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "sns.heatmap(hybrid_train_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Train Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "sns.heatmap(hybrid_test_confusion_matrix, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Test Confusion Matrix')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da1f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('models/hybrid_confusion_matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5afa09f",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b441b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = {\n",
    "    'Model': ['ANN', 'CNN', 'Hybrid'],\n",
    "    'Train Accuracy': [ann_train_accuracy, cnn_train_accuracy, hybrid_train_accuracy],\n",
    "    'Test Accuracy': [ann_test_accuracy, cnn_test_accuracy, hybrid_test_accuracy],\n",
    "    'Train Precision': [ann_train_precision, cnn_train_precision, hybrid_train_precision],\n",
    "    'Test Precision': [ann_test_precision, cnn_test_precision, hybrid_test_precision],\n",
    "    'Train Recall': [ann_train_recall, cnn_train_recall, hybrid_train_recall],\n",
    "    'Test Recall': [ann_test_recall, cnn_test_recall, hybrid_test_recall],\n",
    "    'Train F1 Score': [ann_train_f1, cnn_train_f1, hybrid_train_f1],\n",
    "    'Test F1 Score': [ann_test_f1, cnn_test_f1, hybrid_test_f1]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('model_performance_comparison.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
